---
output:
  html_document:
    keep_md: true
---

So far we've analyzed the Harry Potter series by understanding the frequency and distribution of words across the corpus.  This can be useful in giving context of particular text along with understanding the general sentiment.  However, we often want to understand the relationship between words in a corpus.  What sequences of words are common across our text?  Given a sequence of words, what word is most likely to follow?  What words have the strongest relationship with each other?  These are questions that we will consider in this tutorial.


## tl;dr
This tutorial builds on the [tidy text](tidy_text), [sentiment analysis](sentiment_analysis), and [term vs. document frequency](tf-idf_analysis) tutorials so if you have not read through those tutorials I suggest you start there before proceeding.  In this tutorial I cover the following: 

1. [Replication requirements](#replication): What you’ll need to reproduce the analysis in this tutorial
2. [*n*-gram basics](#ngram): Tokenizing consecutive sequences of words (aka *n*-grams) and assessing *n*-gram frequency
3. [Analyzing *n*-grams](#analyze): Analyzing the tf-idf and sentiment of *n*-grams
4. [Visualizing *n*-gram networks](#visualize): Visualizing the network of relationships among *n*-grams
3. [Word correlation](#corr): Assessing the correlation of words within and across documents



## Replication Requirements {#replication}
This tutorial leverages the data provided in the [`harrypotter` package](https://github.com/bradleyboehmke/harrypotter).  I constructed this package to supply the first seven novels in the Harry Potter series to illustrate text mining and analysis capabilities.  You can load the `harrypotter` package with the following:

```{r, eval=FALSE}
if (packageVersion("devtools") < 1.6) {
  install.packages("devtools")
}

devtools::install_github("bradleyboehmke/harrypotter")
```

```{r, warning=FALSE, message=FALSE, collapse=TRUE}
library(tidyverse)      # data manipulation & plotting
library(stringr)        # text cleaning and regular expressions
library(tidytext)       # provides additional text mining functions
library(harrypotter)    # provides the first seven novels of the Harry Potter series
```

The seven novels we are working with, and are provided by the `harrypotter` package, include:

- `philosophers_stone`: Harry Potter and the Philosophers Stone (1997)
- `chamber_of_secrets`: Harry Potter and the Chamber of Secrets (1998)
- `prisoner_of_azkaban`: Harry Potter and the Prisoner of Azkaban (1999)
- `goblet_of_fire`: Harry Potter and the Goblet of Fire (2000)
- `order_of_the_phoenix`: Harry Potter and the Order of the Phoenix (2003)
- `half_blood_prince`: Harry Potter and the Half-Blood Prince (2005)
- `deathly_hallows`: Harry Potter and the Deathly Hallows (2007)

Each text is in a character vector with each element representing a single chapter.  For instance, the following illustrates the raw text of the first two chapters of the `philosophers_stone`:

```{r, eval=FALSE}
philosophers_stone[1:2]
## [1] "THE BOY WHO LIVED　　Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they 
## were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything 
## strange or mysterious, because they just didn't hold with such nonsense.　　Mr. Dursley was the director of a 
## firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a 
## very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came
## in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The 
## Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.　　The Dursleys 
## had everything they wanted, but they also had a secret, and their greatest fear was that somebody would 
## discover it. They didn't think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. 
## Dursley's sister, but they hadn'... <truncated>
## [2] "THE VANISHING GLASS　　Nearly ten years had passed since the Dursleys had woken up to find their nephew on
## the front step, but Privet Drive had hardly changed at all. The sun rose on the same tidy front gardens and lit
## up the brass number four on the Dursleys' front door; it crept into their living room, which was almost exactly
## the same as it had been on the night when Mr. Dursley had seen that fateful news report about the owls. Only
## the photographs on the mantelpiece really showed how much time had passed. Ten years ago, there had been lots
## of pictures of what looked like a large pink beach ball wearing different-colored bonnets -- but Dudley Dursley
## was no longer a baby, and now the photographs showed a large blond boy riding his first bicycle, on a carousel 
## at the fair, playing a computer game with his father, being hugged and kissed by his mother. The room held no
## sign at all that another boy lived in the house, too.　　Yet Harry Potter was still there, asleep at the
## moment, but no... <truncated>
```


## n-gram Analysis {#ngram}

As we saw in the [tidy text](tidy_text), [sentiment analysis](sentiment_analysis), and [term vs. document frequency](tf-idf_analysis) tutorials we can use the `unnest` function from the `tidytext` package to break up our text by words, paragraphs, etc.  We can also use `unnest` to break up our text by "tokens", aka - a consecutive sequence of words.  These are commonly referred to as *n*-grams where a bi-gram is a pair of two consecutive words, a tri-gram is a group of three consecutive words, etc.  

Here, we follow the same process to prepare our text as we have in the previous three tutorials; however, notice that in the `unnest` function I apply a `token` argument to state we want *n*-grams and the `n = 2` tells it we want bi-grams.

```{r, collapse=TRUE, warning=FALSE, message=FALSE}
titles <- c("Philosopher's Stone", "Chamber of Secrets", "Prisoner of Azkaban",
            "Goblet of Fire", "Order of the Phoenix", "Half-Blood Prince",
            "Deathly Hallows")

books <- list(philosophers_stone, chamber_of_secrets, prisoner_of_azkaban,
           goblet_of_fire, order_of_the_phoenix, half_blood_prince,
           deathly_hallows)
  
series <- tibble()

for(i in seq_along(titles)) {
        
        clean <- tibble(chapter = seq_along(books[[i]]),
                        text = books[[i]]) %>%
             unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
             mutate(book = titles[i]) %>%
             select(book, everything())

        series <- rbind(series, clean)
}

# set factor to keep books in order of publication
series$book <- factor(series$book, levels = rev(titles))

series
```


Our output is similar to what we had in the previous tutorials; however, note that our bi-grams have groups of two words.  Also, note how there is some repetition, or overlapping.  The sentence "The boy who lived" Is broken up into 3 bi-grams:

- "the boy"
- "boy who"
- "who lived"

This is done for the entire Harry Potter series and captures all the sequences of two consecutive words.  We can now perform common frequency analysis procedures.  First, let's look at the most common bi-grams across the entire Harry Potter series:

```{r, collapse=TRUE, warning=FALSE, message=FALSE}
series %>%
        count(bigram, sort = TRUE)
```

With the exception of "said harry" the most common bi-grams include very common words that do not provide much context.  We can filter out these common *stop* words to find the most common bi-grams that provide context.  The results show pairs of words that are far more contextual than our previous set.

```{r, collapse=TRUE, warning=FALSE, message=FALSE}
series %>%
        separate(bigram, c("word1", "word2"), sep = " ") %>%
        filter(!word1 %in% stop_words$word,
               !word2 %in% stop_words$word) %>%
        count(word1, word2, sort = TRUE)
```


Similar to the previous text mining tutorials we can visualize the top 10 bi-grams for each book. 

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=9, fig.height=8}
series %>%
        separate(bigram, c("word1", "word2"), sep = " ") %>%
        filter(!word1 %in% stop_words$word,
               !word2 %in% stop_words$word) %>%
        count(book, word1, word2, sort = TRUE) %>%
        unite("bigram", c(word1, word2), sep = " ") %>%
        group_by(book) %>%
        top_n(10) %>%
        ungroup() %>%
        mutate(book = factor(book) %>% forcats::fct_rev()) %>%
        ggplot(aes(drlib::reorder_within(bigram, n, book), n, fill = book)) +
        geom_bar(stat = "identity", alpha = .8, show.legend = FALSE) +
        drlib::scale_x_reordered() +
        facet_wrap(~ book, ncol = 2, scales = "free") +
        coord_flip()
```

## Analyzing n-grams {#analyze}

We can also follow a similar process as performed in the [term vs. document frequency](tf-idf_analysis) tutorial to identify the tf-idf of *n*-grams (or bi-grams in our ongoing example).

```{r, collapse=TRUE, warning=FALSE, message=FALSE}
(bigram_tf_idf <- series %>%
        count(book, bigram, sort = TRUE) %>%
        bind_tf_idf(bigram, book, n) %>%
        arrange(desc(tf_idf))
)
```

And we can visualize the bigrams with the highest tf_idf for each book:

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=10, fig.height=12}
bigram_tf_idf %>%
        group_by(book) %>%
        top_n(15, wt = tf_idf) %>%
        ungroup() %>%
        mutate(book = factor(book) %>% forcats::fct_rev()) %>%
        ggplot(aes(drlib::reorder_within(bigram, tf_idf, book), tf_idf, fill = book)) +
        geom_bar(stat = "identity", alpha = .8, show.legend = FALSE) +
        labs(title = "Highest tf-idf bi-grams in the Harry Potter series",
             x = NULL, y = "tf-idf") +
        drlib::scale_x_reordered() +
        facet_wrap(~book, ncol = 2, scales = "free") +
        coord_flip()
```

The sentiment analysis approch used in the [sentiment analysis](sentiment_analysis) tutorial simply counted the appearance of positive or negative words, according to a specified lexicon (i.e. AFINN, bing, nrc). Unfortunately, this approach scores the sentiments of words merely on their presence rather than on context. For example, the words "happy" and "like" will be counted as positive, even in a sentence like "I'm not happy and I don’t like it!"   

By performing sentiment analysis on our bi-gram data, we can examine how often sentiment-associated words are preceded by "not" or other negating words. 

```{r, collapse=TRUE, message=FALSE, warning=FALSE}
series %>%
        separate(bigram, c("word1", "word2"), sep = " ") %>%
        filter(word1 == "not") %>%
        count(book, word1, word2, sort = TRUE)
```


We could use this to ignore or even reverse their contribution to the sentiment score.  Here we'll use the AFINN lexicon for the sentiment analysis, which gives a numeric sentiment score for each word. We can assess the most frequent words that have a sentiment score and were preceded by "not".

```{r, collapse=TRUE, message=FALSE, warning=FALSE}
AFINN <- get_sentiments("afinn")

(nots <- series %>%
        separate(bigram, c("word1", "word2"), sep = " ") %>%
        filter(word1 == "not") %>%
        inner_join(AFINN, by = c(word2 = "word")) %>%
        count(word2, score, sort = TRUE) 
)
```

We can use this information to see the total impact these cases had on misspecifying sentiment.  For example, above we saw that the top two words preceded by "not" was "want" and "help".  The sentiment score for "want" is +1; however, "want" was preceded by "not" 81 times which means the sentiment could easily have been overstated by $$81 \times 1 = 81$$ points. "help" on the otherhand has a sentiment score of +2 but was preceded by "not" 45 times which means the sentiment could have been overstated by $$45 \times 2 = 90$$ points for these instances. 

```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=8, fig.height=5}
nots %>%
        mutate(contribution = n * score) %>%
        arrange(desc(abs(contribution))) %>%
        head(20) %>%
        ggplot(aes(reorder(word2, contribution), n * score, fill = n * score > 0)) +
        geom_bar(stat = "identity", show.legend = FALSE) +
        xlab("Words preceded by 'not'") +
        ylab("Sentiment score * # of occurrances") +
        coord_flip()
```

The bi-grams "not help", "not want", "not like" were the largest causes of misidentification, making the text seem much more positive than it is. We also see bi-grams such as "not bad", "not dead", and "not stupid" as the bottom of our chart suggesting these bi-grams made the text appear more negative than it is.

We could expand this example and use a full list of words that signal negation (i.e. "not", "no", "never", "without"). This would allow us to find a larger pool of words that are preceded by negation and identify their impact on sentiment analysis.

```{r, collapse=TRUE, message=FALSE, warning=FALSE}
negation_words <- c("not", "no", "never", "without")

(negated <- series %>%
                separate(bigram, c("word1", "word2"), sep = " ") %>%
                filter(word1 %in% negation_words) %>%
                inner_join(AFINN, by = c(word2 = "word")) %>%
                count(word1, word2, score, sort = TRUE) %>%
                ungroup()
)
```


```{r, warning=FALSE, message=FALSE, fig.align='center', fig.width=10, fig.height=10}
negated %>%
        mutate(contribution = n * score) %>%
        arrange(desc(abs(contribution))) %>%
        group_by(word1) %>%
        top_n(10, abs(contribution)) %>%
        ggplot(aes(drlib::reorder_within(word2, contribution, word1), contribution, fill = contribution > 0)) +
        geom_bar(stat = "identity", show.legend = FALSE) +
        xlab("Words preceded by 'not'") +
        ylab("Sentiment score * # of occurrances") +
        drlib::scale_x_reordered() +
        facet_wrap(~ word1, scales = "free") +
        coord_flip()
```

## Visualizing n-gram Networks {#visualize}

## Word Correlation {#corr}






